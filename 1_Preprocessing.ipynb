{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ee584b-90e8-4c71-a74c-f4a937e97e9b",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff791d3-1458-401e-93d3-32e46aa18d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9f6f3-5f78-434e-be2d-4a24233ef1f2",
   "metadata": {},
   "source": [
    "## Define functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f411f3-7651-434b-9a03-c530798d9bd3",
   "metadata": {},
   "source": [
    "### 1. Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e1297c-c6d4-4011-8a1f-28cb6672361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_tmp = pd.read_csv('./Data/Train.csv') # read in train temporarily to find all the emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9cc26-64d6-47b6-94dc-76d5f9c768d6",
   "metadata": {},
   "source": [
    "Emoticon search process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42fe25c5-84c7-4249-af9b-4571d34ea0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, get a set of all the words that have appeared in the tweets\n",
    "single_words = set()\n",
    "for tweet in Train_tmp['text']:\n",
    "    for word in tweet.split(' '):\n",
    "        single_words.add(word)\n",
    "\n",
    "single_words = list(single_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48cb79ff-140a-40ab-bb3f-ff133142fc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(:', '):', ');', ':(', ':)', ':-(', ':-)', ';(', ';)', ';-)'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find emoticons\n",
    "emoticons = set()\n",
    "for word in single_words:\n",
    "    for elem in re.findall(r'[:;]-{0,1}[()]', word):\n",
    "        emoticons.add(elem)\n",
    "        \n",
    "    for elem in re.findall(r'[()]-{0,1}[:;]', word):\n",
    "        emoticons.add(elem)\n",
    "        \n",
    "    for elem in re.findall(r'[:;][^[a-zA-Z]]{1,2}', word):\n",
    "        emoticons.add(elem)\n",
    "        \n",
    "    for elem in re.findall(r'[^[a-zA-Z]]{1,2}[:;]', word):\n",
    "        emoticons.add(elem)\n",
    "\n",
    "emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70da83b-04cd-46c3-b0dc-d8205332b068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'', ':(', ':)', ':-/', ':/', ':p', ';(', ';)', ';p', '<3'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find more emoticons\n",
    "# source: https://stackoverflow.com/questions/28077049/regex-matching-emoticons; added my own to the regex term\n",
    "emoticons2 = set()\n",
    "for word in single_words:\n",
    "    for elem in re.findall(r'(\\:\\)|\\:\\(|<3|\\:\\/|\\:-\\/|\\:\\||\\:p|\\:P|\\;\\)|\\;\\(|<3|\\;\\/|\\;-\\/|\\;\\||\\;p|\\;P|)', word):\n",
    "        emoticons2.add(elem)\n",
    "emoticons2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed15b61-5359-4397-8c92-36b69ace442e",
   "metadata": {},
   "source": [
    "Keep in mind: We need to do same process for train cases too - so final set of emoticons more than what was picked up.\n",
    "\n",
    "Didn't deal with :0, :p etc because they were ambiguous and also could mess up actual words \n",
    "\n",
    "Also, assume that all words are seperated by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e62498-fef9-4c61-91c3-0c8774554d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticons(df):\n",
    "    \n",
    "    \"\"\" Changes emoticons into designated tags denoting positive or negative emoticons \"\"\"\n",
    "    \n",
    "    posEmotic = ['(:', ':)', ':-)', '<3', ';)', '(;', ';-)']\n",
    "    negEmotic = ['):', ':(', ':-(', ':-/', ':/', ');', ';(', ';-(', ';-/']\n",
    "\n",
    "    newText = list()\n",
    "\n",
    "    for tweet in df['text']:\n",
    "        out = tweet\n",
    "\n",
    "        trigger = False\n",
    "\n",
    "        for etc in posEmotic:\n",
    "            if len(out.split(etc)) > 1:\n",
    "\n",
    "                trigger = True\n",
    "\n",
    "                # print(out)\n",
    "                # print('\\n')\n",
    "\n",
    "                splitted = out.split(etc)\n",
    "\n",
    "                # print(splitted)\n",
    "                # print('\\n')\n",
    "\n",
    "                tmp = str()\n",
    "\n",
    "                for j in range(len(splitted)-1):\n",
    "                    tmp += splitted[j]\n",
    "                    tmp += ' PPOSEMOTICONN '\n",
    "                tmp += splitted[-1]\n",
    "\n",
    "                out = tmp\n",
    "\n",
    "                # print(out)\n",
    "                # print('\\n')\n",
    "\n",
    "        for etc in negEmotic:\n",
    "            if len(out.split(etc)) > 1:\n",
    "\n",
    "                # print(out)\n",
    "                # print('\\n')\n",
    "\n",
    "                splitted = out.split(etc)\n",
    "\n",
    "                # print(splitted)\n",
    "                # print('\\n')\n",
    "\n",
    "                tmp = str()\n",
    "\n",
    "                for j in range(len(splitted)-1):\n",
    "                    tmp += splitted[j]\n",
    "                    tmp += ' NNEGEMOTICONN '\n",
    "                tmp += splitted[-1]\n",
    "\n",
    "                out = tmp\n",
    "\n",
    "                # print(out)\n",
    "                # print('\\n')\n",
    "\n",
    "        newText.append(out)\n",
    "\n",
    "    df['text'] = newText\n",
    "\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1afd5f59-8495-4cb3-b259-47a1fd744bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_mix_emoticons(df):\n",
    "    \"\"\" Remove all emoticons if both positive and negative emoticons appear in a tweet \"\"\"\n",
    "    \n",
    "    newText = list()\n",
    "\n",
    "    for tweet in df['text']:\n",
    "\n",
    "        if 'PPOSEMOTICONN' in tweet and 'NNEGEMOTICONN' in tweet:\n",
    "            regex = re.compile(r'(PPOSEMOTICONN|NNEGEMOTICONN)')\n",
    "            tweet = regex.sub('', tweet)\n",
    "\n",
    "        newText.append(tweet)\n",
    "\n",
    "    df['text'] = newText\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c24bcb-5acd-4974-9db1-8e3286f7733a",
   "metadata": {},
   "source": [
    "### 2. Retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f0a49f-f8f3-482b-813e-cfce713edae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_retweet(df):\n",
    "    \n",
    "    newText = list()\n",
    "    \n",
    "    for tweet in df['text']:\n",
    "        regex = re.compile(r'^[ ]?rt @')\n",
    "        if re.search(r'^[ ]?rt @', tweet):\n",
    "            # all text are lower case!!\n",
    "            tweet = regex.sub(r'@', tweet)\n",
    "        newText.append(tweet)\n",
    "    df['text'] = newText\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216a992-3008-44fc-9240-f523775313e7",
   "metadata": {},
   "source": [
    "### 3. Replace username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df17d2d-5022-43f9-8cb2-abbc89f5f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_username(df):\n",
    "    for i in range(len(df['text'])):\n",
    "        df['text'][i] = re.sub(r\"\\B@\\w+\", \"\", df['text'][i])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91648a7-30bf-4b61-a4d8-67e4755c8a32",
   "metadata": {},
   "source": [
    "### 4. Replace HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19e138dc-5bc2-415d-aac9-948188cb63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_htmls(df):\n",
    "    for i in range(len(df['text'])):\n",
    "        df['text'][i] = re.sub(r\"http[^\\s]+\", \"\", df['text'][i])  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bf714-b027-417e-ad58-d785d97fc1d1",
   "metadata": {},
   "source": [
    "### 5. Replace repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b66a6f8-2c26-423b-93c7-3d4d9edfa692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_repeated_letters(df):\n",
    "    \n",
    "    for i in range(len(df['text'])):\n",
    "        if re.match(r\"([a-zA-Z])\\1\\1+\", df['text'][i]):\n",
    "            df['text'][i] = re.sub(r\"([a-zA-Z])\\1\\1+\", r\"\\1\\1\", df['text'][i])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c735530-4860-4f49-90fd-7b199290ea78",
   "metadata": {},
   "source": [
    "### 6. Remove Duplicated Tweet\n",
    "\n",
    "**note: test cases DON'T need to do this step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff7d7221-6135-4686-9b14-9d31f81fae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicated_tweet(df):\n",
    "    df = df.drop_duplicates(subset='text', keep='first')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4bf5f-7e30-4e89-b0d7-744e9c217a2f",
   "metadata": {},
   "source": [
    "### 7. Unify Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a277edd9-9fd1-4671-93a7-873b68860ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_numeric(df):\n",
    "\n",
    "    tmp = list()\n",
    "    for tweet in df['text']:\n",
    "        tmp.append(re.sub(r'\\d+', '', tweet))\n",
    "    \n",
    "    df['text'] = tmp\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04133213-c2a5-4a6e-80b1-ee5291dccd73",
   "metadata": {},
   "source": [
    "### 8. Unify Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48d116fb-1d76-4c29-92a5-a24831542af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_percentage(df):\n",
    "    \n",
    "    tmp = list()\n",
    "    for tweet in df['text']:\n",
    "        tmp.append(re.sub(r'\\d+[ ]?(percent|%)', '', tweet))\n",
    "    \n",
    "    df['text'] = tmp\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f2de4-8884-4269-b062-39fce90371f6",
   "metadata": {},
   "source": [
    "### 9. Expand contraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad0e4082-0803-46a4-b27a-d6891e924c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contraction(df):\n",
    "    df['text'] = df.text.apply(contractions_handel)\n",
    "    return df\n",
    "\n",
    "def contractions_handel(text):\n",
    "    import contractions\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc44fec-baa1-4da4-84f8-466d62217144",
   "metadata": {},
   "source": [
    "## Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae8f4893-bbc2-4ebb-8a0a-d82dcdd78adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, mode):\n",
    "\n",
    "    df = replace_htmls(df)\n",
    "\n",
    "    df = replace_retweet(df)\n",
    "    \n",
    "    if mode == 'Train':\n",
    "        df = remove_duplicated_tweet(df)\n",
    "\n",
    "    df = replace_username(df)\n",
    "\n",
    "    df = emoticons(df)\n",
    "    df = rm_mix_emoticons(df)\n",
    "\n",
    "    df = unify_percentage(df)\n",
    "\n",
    "    df = unify_numeric(df)\n",
    "\n",
    "    df = replace_repeated_letters(df)\n",
    "\n",
    "    # df = expand_contraction(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec367607-9596-4700-bc76-2c8b85340744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_7744/2216076501.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i] = re.sub(r\"http[^\\s]+\", \"\", df['text'][i])\n"
     ]
    }
   ],
   "source": [
    "# Train and output\n",
    "Train = pd.read_csv('./Data/Train.csv')\n",
    "Future = pd.read_csv('./Data/Future.csv')\n",
    "\n",
    "Train = pipeline(Train, 'Train')\n",
    "Future = pipeline(Future, 'Future')\n",
    "\n",
    "Train.to_csv('./Data/Train_Preprocessed.csv', index = False)\n",
    "Future.to_csv('./Data/Future_Preprocessed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea3b6a-fa9e-4261-ab2b-ed640c44e920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
