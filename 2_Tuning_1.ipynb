{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8f6eb0-6f44-4530-9222-e097965c5dbb",
   "metadata": {},
   "source": [
    "# 2. Tuning Part 1\n",
    "\n",
    "### for SVM and Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d6a77b-f110-42a2-959f-866989fe4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import BernoulliNB as BNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaffa39f-08a8-4eb6-ae7b-864429ff1933",
   "metadata": {},
   "source": [
    "# Clean and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a178e44-2fc3-4805-81dc-5517c117f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv('./Data/Train_Preprocessed.csv')\n",
    "Future = pd.read_csv('./Data/Future_Preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82416f10-4d40-44c0-9128-b0ef22229eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndSplit(df, mode):\n",
    "    \n",
    "    text_list = list()\n",
    "    \n",
    "    for tweet in df['text']:\n",
    "\n",
    "        tmp = list()\n",
    "\n",
    "        if mode == 'NoHashtag':\n",
    "            tweet = re.sub(r'[^\\w%\\'-]', ' ', tweet)\n",
    "\n",
    "            # get rid of non-english characters\n",
    "            tweet = re.sub(r'[^\\x00-\\x7F ]+', ' ', tweet) \n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # get rid of non-alphanumerical chars, effectively splitting them\n",
    "            # except for at ' - # and %\n",
    "            tweet = re.sub(r'[^\\x00-\\x7F#%\\'-]', ' ', tweet)\n",
    "\n",
    "\n",
    "        # split the words based on space\n",
    "        words = tweet.split()\n",
    "\n",
    "        for word in words:\n",
    "\n",
    "            # get rid of ' and % and - that are outside of words\n",
    "            word = word.strip('\\'%-')\n",
    "            \n",
    "            # gets rid of other noise\n",
    "            if mode == 'NoHashtag':\n",
    "                word = re.sub(r'[^-\\w\\']', '', word)\n",
    "            \n",
    "            else:\n",
    "                word = re.sub(r'[^\\w#-\\']', '', word)\n",
    "\n",
    "\n",
    "            if word not in ['', '#']:\n",
    "                tmp.append(word)\n",
    "                \n",
    "        text_list.append(tmp)\n",
    "    \n",
    "    return text_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41c0e8f-0b70-4312-916f-2ab426ee51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainWords = Train.copy(deep=True)\n",
    "TrainWords['text'] = cleanAndSplit(Train, 'Hashtag')\n",
    "\n",
    "FutureWords = Future.copy(deep=True)\n",
    "FutureWords['text'] = cleanAndSplit(Future, 'Hashtag')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f96eb-167e-4716-9dcd-2d2a198f6e69",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2c4a79-6c73-481b-beae-7b9926402d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    return [ps.stem(w) for w in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cef51-2c69-4d5c-9caa-03a4189997eb",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a6a5359-677c-4b62-a75d-12af144cbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     return [w for w in text if not w in stop_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca5995f-f7da-486c-9211-5b5cf5507a4d",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec2c510-8b1c-4da3-b34c-715443b066fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_string(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "Y_PreproTFIDF = TrainWords['sentiment'] # because drop one duplicate \n",
    "\n",
    "\n",
    "## Preprocess TFIDF\n",
    "# Train\n",
    "TrainPreproTFIDF = TrainWords.copy(deep = True)\n",
    "TrainPreproTFIDF['text'] = TrainPreproTFIDF['text'].apply(stem_text)\n",
    "TrainPreproTFIDF['text'] = TrainPreproTFIDF['text'].apply(join_string)\n",
    "tfidf_vectorizer_Prepro = TfidfVectorizer(lowercase=False, ngram_range=(1,1))\n",
    "X_PreproTFIDF = tfidf_vectorizer_Prepro.fit_transform(TrainPreproTFIDF['text'])\n",
    "\n",
    "\n",
    "# Future\n",
    "FuturePreproTFIDF = FutureWords.copy(deep = True)\n",
    "FuturePreproTFIDF['text'] = FuturePreproTFIDF['text'].apply(stem_text)\n",
    "FuturePreproTFIDF['text'] = FuturePreproTFIDF['text'].apply(join_string)\n",
    "X_Future_PreproTFIDF = tfidf_vectorizer_Prepro.transform(FuturePreproTFIDF['text'])\n",
    "\n",
    "\n",
    "\n",
    "# ##UNPROCESSED only TFIDF\n",
    "# # Train\n",
    "# TrainTFIDF = pd.read_csv(\"./Data/Train.csv\")\n",
    "\n",
    "# Y_TFIDF = TrainTFIDF['sentiment']\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(lowercase=False, ngram_range=(1,1))\n",
    "# X_TrainTFIDF = tfidf_vectorizer.fit_transform(TrainTFIDF['text'])\n",
    "\n",
    "# # Future\n",
    "# FutureTFIDF = pd.read_csv(\"./Data/Future.csv\")\n",
    "# X_Future_TFIDF = tfidf_vectorizer.transform(FutureTFIDF['text'])\n",
    "\n",
    "\n",
    "\n",
    "# ##UNPROCESSED TFIDF + rid junk\n",
    "# # Train\n",
    "# TrainNoJunkTFIDF = TrainTFIDF.copy(deep = True)\n",
    "# TrainNoJunkTFIDF['text'] = cleanAndSplit(TrainNoJunkTFIDF, 'Hashtag')\n",
    "# tfidf_vectorizer_NoJunk = TfidfVectorizer(lowercase=False, ngram_range=(1,1))\n",
    "# TrainNoJunkTFIDF['text'] = TrainNoJunkTFIDF['text'].apply(join_string)\n",
    "# X_TrainNoJunkTFIDF = tfidf_vectorizer_NoJunk.fit_transform(TrainNoJunkTFIDF['text'])\n",
    "\n",
    "\n",
    "# # Future\n",
    "# FutureNoJunkTFIDF = FutureTFIDF.copy(deep = True)\n",
    "# FutureNoJunkTFIDF['text'] = cleanAndSplit(FutureNoJunkTFIDF, 'Hashtag')\n",
    "# FutureNoJunkTFIDF['text'] = FutureNoJunkTFIDF['text'].apply(join_string)\n",
    "# X_Future_NoJunkTFIDF = tfidf_vectorizer_NoJunk.transform(FutureNoJunkTFIDF['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # W2V\n",
    "# TrainW2V = pd.read_csv('./Data/W300V_Train.csv')\n",
    "\n",
    "# FutureW2V = pd.read_csv('./Data/W300V_Future.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6cc1f-62cc-4840-a05e-b94bc5de41db",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084eda5d-86ef-4a10-b7de-c450265201a3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2eb57c-7b06-4811-bb8b-f8faf55fabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    eval_ = pd.DataFrame({'p': y_pred, 't': y_test})\n",
    "    \n",
    "    y_test = list(y_test)\n",
    "    \n",
    "    n = len(y_test)\n",
    "    n1 = sum([1 for i in range(len(y_test)) if y_test[i] == 'positive'])\n",
    "    n2 = sum([1 for i in range(len(y_test)) if y_test[i] == 'neutral'])\n",
    "    n3 = sum([1 for i in range(len(y_test)) if y_test[i] == 'negative'])\n",
    "    \n",
    "    predPosd = [ len(eval_[(eval_['p'] == 'positive') & (eval_['t'] == 'positive')])/n,\n",
    "               len(eval_[(eval_['p'] == 'positive') & (eval_['t'] == 'neutral')])/n,\n",
    "               len(eval_[(eval_['p'] == 'positive') & (eval_['t'] == 'negative')])/n\n",
    "              ]\n",
    "    predNeud = [ len(eval_[(eval_['p'] == 'neutral') & (eval_['t'] == 'positive')])/n,\n",
    "               len(eval_[(eval_['p'] == 'neutral') & (eval_['t'] == 'neutral')])/n,\n",
    "               len(eval_[(eval_['p'] == 'neutral') & (eval_['t'] == 'negative')])/n\n",
    "              ]\n",
    "    predNegd = [ len(eval_[(eval_['p'] == 'negative') & (eval_['t'] == 'positive')])/n,\n",
    "               len(eval_[(eval_['p'] == 'negative') & (eval_['t'] == 'neutral')])/n,\n",
    "               len(eval_[(eval_['p'] == 'negative') & (eval_['t'] == 'negative')])/n\n",
    "              ]\n",
    "    \n",
    "    predPos = [ len(eval_[(eval_['p'] == 'positive') & (eval_['t'] == 'positive')]),\n",
    "               len(eval_[(eval_['p'] == 'positive') & (eval_['t'] == 'neutral')]),\n",
    "               len(eval_[(eval_['p'] == 'positive') & (eval_['t'] == 'negative')])\n",
    "              ]\n",
    "    predNeu = [ len(eval_[(eval_['p'] == 'neutral') & (eval_['t'] == 'positive')]),\n",
    "               len(eval_[(eval_['p'] == 'neutral') & (eval_['t'] == 'neutral')]),\n",
    "               len(eval_[(eval_['p'] == 'neutral') & (eval_['t'] == 'negative')])\n",
    "              ]\n",
    "    predNeg = [ len(eval_[(eval_['p'] == 'negative') & (eval_['t'] == 'positive')]),\n",
    "               len(eval_[(eval_['p'] == 'negative') & (eval_['t'] == 'neutral')]),\n",
    "               len(eval_[(eval_['p'] == 'negative') & (eval_['t'] == 'negative')])\n",
    "              ]\n",
    "    \n",
    "    accuracy = predPos[0] + predNeu[1] + predNeg[2]\n",
    "    \n",
    "    error_rate = 1-accuracy\n",
    "    \n",
    "    confus_matrix = pd.DataFrame({'predPos': predPos, 'predNeu': predNeu, 'predNeg': predNeg}, \n",
    "                                 index = ['truePos', 'trueNeu', 'trueNeg'])\n",
    "    confus_matrix_d = pd.DataFrame({'predPos': predPosd, 'predNeu': predNeud, 'predNeg': predNegd}, \n",
    "                                 index = ['truePos', 'trueNeu', 'trueNeg'])\n",
    "    \n",
    "    # error_reduction_rate = error_rate - \n",
    "    \n",
    "    precision1 = 0 if (sum(predPos)) == 0 else predPos[0]/(sum(predPos))\n",
    "    precision2 = 0 if (sum(predNeu)) == 0 else predNeu[1]/(sum(predNeu))\n",
    "    precision3 = 0 if (sum(predNeg)) == 0 else predNeg[2]/(sum(predNeg))\n",
    "    \n",
    "    recall1 = 0 if (predPos[0]+predNeu[0]+predNeg[0]) == 0 else predPos[0]/(predPos[0]+predNeu[0]+predNeg[0])\n",
    "    recall2 = 0 if (predPos[1]+predNeu[1]+predNeg[1]) == 0 else predNeu[1]/(predPos[1]+predNeu[1]+predNeg[1])\n",
    "    recall3 = 0 if (predPos[2]+predNeu[2]+predNeg[2]) == 0 else predNeg[2]/(predPos[2]+predNeu[2]+predNeg[2])\n",
    "    \n",
    "    f11 = 0 if (precision1+recall1) == 0 else (2*precision1*recall1)/(precision1+recall1)\n",
    "    f12 = 0 if (precision2+recall2) == 0 else (2*precision2*recall2)/(precision2+recall2)\n",
    "    f13 = 0 if (precision3+recall3) == 0 else (2*precision3*recall3)/(precision3+recall3)\n",
    "    \n",
    "    # specificity1 = \n",
    "    # specificity2 = \n",
    "    # specificity3 = \n",
    "    \n",
    "    col1 = [precision1, recall1, f11]\n",
    "    col2 = [precision2, recall2, f12]\n",
    "    col3 = [precision3, recall3, f13]\n",
    "    \n",
    "    precision = (n1/n)*precision1 + (n2/n)*precision2 + (n3/n)*precision3\n",
    "    recall = (n1/n)*recall1 + (n2/n)*recall2 + (n3/n)*recall3\n",
    "    f1 = (n1/n)*f11 + (n2/n)*f12 + (n3/n)*f13\n",
    "    \n",
    "    scores = pd.DataFrame({'Pos': col1, 'Neu': col2, 'Neg': col3}, index = ['precision', 'recall', 'f1'])\n",
    "    \n",
    "    return confus_matrix, confus_matrix_d, scores, accuracy/n, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a06fa-dd57-4491-b888-7242469f5f69",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f7d3f0-4c56-45d7-8394-9f547335b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = copy.deepcopy(X_PreproTFIDF) # 有preprocess，TFIDF\n",
    "Y_ = copy.deepcopy(Y_PreproTFIDF) # 1 用\n",
    "\n",
    "# X_ = copy.deepcopy(X_TrainTFIDF) # 无preprocess，TFIDF\n",
    "\n",
    "# X_ = copy.deepcopy(X_TrainNoJunkTFIDF) # 无preprocess，稍微清理了一下标点和不规则符号\n",
    "\n",
    "# Y_ = copy.deepcopy(Y_TFIDF) # 2 和 3 用\n",
    "\n",
    "\n",
    "# X_ = copy.deepcopy(TrainW2V) # W2V，你可以ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ca3f0-436b-44f2-9e5b-5205673924fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_F = copy.deepcopy(X_Future_PreproTFIDF) # 有preprocess，TFIDF\n",
    "\n",
    "# X_F = copy.deepcopy(X_Future_TFIDF) # 无preprocess，TFIDF\n",
    "\n",
    "X_F = copy.deepcopy(X_Future_NoJunkTFIDF) # 无preprocess，稍微清理了一下标点和不规则符号\n",
    "\n",
    "# X_F = copy.deepcopy(FutureW2V) # W2V，你可以ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff7dac-6005-49c7-ba7a-3f689eb78ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Future = pd.read_csv('./Data/Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd3391-b0c1-4551-ab86-a7a25f511b41",
   "metadata": {},
   "source": [
    "## 0. 0R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5be7f-a818-498a-a526-531073cfca6f",
   "metadata": {},
   "source": [
    "Just run one (Probably should not do train test split, but this was done as a model case for the rest to follow - also probablistically speaking should not make a huge difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0780931-0d5e-4252-b2e4-1f3a55244ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_0R = pd.read_csv('./Data/Train.csv')\n",
    "\n",
    "X_0R = Train_0R['text']\n",
    "y_0R = Train_0R['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ee7e071-9c82-47a0-b7ec-5194c3a25680",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroR = DummyClassifier(strategy = \"most_frequent\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_0R, y_0R, train_size = 0.8, random_state = 30027)\n",
    "ZeroR.fit(X_train, y_train)\n",
    "y_pred = ZeroR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8d6187-7421-4545-a861-9e5683c26157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ZeroR, y_pred, y_test, labels = ['negative', 'neutral', 'positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd2d5ac7-f84a-4ce4-a896-7b35daed9b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predPos</th>\n",
       "      <th>predNeu</th>\n",
       "      <th>predNeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truePos</th>\n",
       "      <td>0</td>\n",
       "      <td>1112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueNeu</th>\n",
       "      <td>0</td>\n",
       "      <td>2539</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueNeg</th>\n",
       "      <td>0</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predPos  predNeu  predNeg\n",
       "truePos        0     1112        0\n",
       "trueNeu        0     2539        0\n",
       "trueNeg        0      710        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Decimal: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predPos</th>\n",
       "      <th>predNeu</th>\n",
       "      <th>predNeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truePos</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254987</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueNeu</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582206</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueNeg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162807</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predPos   predNeu  predNeg\n",
       "truePos      0.0  0.254987      0.0\n",
       "trueNeu      0.0  0.582206      0.0\n",
       "trueNeg      0.0  0.162807      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Matrix: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neu</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582206</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pos       Neu  Neg\n",
       "precision  0.0  0.582206  0.0\n",
       "recall     0.0  1.000000  0.0\n",
       "f1         0.0  0.735942  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5822059160742948\n",
      "Precision:  0.33896372871190883\n",
      "Recall:  0.5822059160742948\n",
      "f1:  0.42846980316308253\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_matrix, c_matrix_d, score, accuracy, precision, recall, f1 = evaluate(y_test, y_pred)\n",
    "print('Confusion Matrix: ')\n",
    "display(c_matrix)\n",
    "print('Confusion Matrix Decimal: ')\n",
    "display(c_matrix_d)\n",
    "print('Score Matrix: ')\n",
    "display(score)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('f1: ', f1)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c6b14-ee00-401c-b322-a705f8ae271f",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c42d9fa-3266-4407-ae98-f50aa34f8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits = k, shuffle = True, random_state = 42)\n",
    "acc_score = []\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "\n",
    "for train_index, test_index in kf.split(Train_0R):\n",
    "    X_train, X_test = X_0R.iloc[train_index], X_0R.iloc[test_index]\n",
    "    y_train, y_test = y_0R.iloc[train_index], y_0R.iloc[test_index]\n",
    "    \n",
    "    ZeroR = DummyClassifier(strategy = \"most_frequent\")\n",
    "    ZeroR.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = ZeroR.predict(X_test)\n",
    "    \n",
    "    c_matrix, c_matrix_d, score, accuracy, precision, recall, f1 = evaluate(y_test, y_pred)\n",
    "    \n",
    "    acc_score.append(accuracy_score(y_test, y_pred))\n",
    "    precision_score.append(precision)\n",
    "    recall_score.append(recall)\n",
    "    f1_score.append(f1)\n",
    "    \n",
    "    # print('Confusion Matrix: ')\n",
    "    # display(c_matrix)\n",
    "    # print('Confusion Matrix Decimal: ')\n",
    "    # display(c_matrix_d)\n",
    "    # print('Score Matrix: ')\n",
    "    # display(score)\n",
    "    # print('Accuracy: ', accuracy)\n",
    "    # print('Precision: ', precision)\n",
    "    # print('Recall: ', recall)\n",
    "    # print('f1: ', f1)\n",
    "    # print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe02c80c-0b4f-47bf-a989-a03999823b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5812886952533822,\n",
       " 0.5819766108690667,\n",
       " 0.5759174311926606,\n",
       " 0.5866972477064221,\n",
       " 0.5772935779816514]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be561342-a5ba-4b15-84d7-53d472ec8d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5806347126006366\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8eaeae26-409c-4027-a72e-7e98ae3ed4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4479728121201361e-05\n"
     ]
    }
   ],
   "source": [
    "print(np.var(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e7b9720-46ae-4fb4-8d72-286672ebf0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0038052237938393796\n"
     ]
    }
   ],
   "source": [
    "print(np.std(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0ffba21b-c4e7-44bc-958a-40f21e33fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33715114920494504\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(precision_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac8d6ecc-0101-4af0-9cf1-64cceb463d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5806347126006366\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1dd2f1e-930a-4fcc-b48d-b3f0dd9e56b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4265912398638325\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "26400c6c-0466-4c4b-bf36-3e3f7d069b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Future_0R = pd.read_csv('./Data/Future.csv')\n",
    "ypred_0R_Future = ZeroR.predict(Future_0R)\n",
    "\n",
    "out_0R = copy.deepcopy(Future_0R)\n",
    "out_0R = out_0R.drop(columns = ['text'])\n",
    "out_0R['sentiment'] = ypred_0R_Future\n",
    "\n",
    "out_0R.to_csv('./Predictions/0R.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d1fe5-0b73-43a6-8c43-2604c674193b",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa9044-0112-4096-88b6-43669bb77285",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Run = LogisticRegression(max_iter = 5000, penalty = 'l1', solver = 'saga', C = 2, multi_class = 'multinomial')\n",
    "\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X_, Y_, train_size = 0.7, random_state = 30027)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "\n",
    "LR_Run.fit(X_train, y_train)\n",
    "accuracy_score(y_val, LR_Run.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c6090-8275-42ef-9398-58fb790bf626",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, LR_Run.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80362c43-c7d0-4958-9b02-d49a2ff07fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, LR_Run.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98c07f-27ce-4386-8bb7-d4499df1851f",
   "metadata": {},
   "source": [
    "With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c6fd05f-95b9-4a95-909b-48d95014fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'LR'\n",
    "DTYPE = 'TFIDF_NoJunk'\n",
    "\n",
    "cent_storage_cols = {'C': [], 'penalty': [], 'solver': [], 'fs':[], 'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f9fdc4-fcc9-4706-a94e-54d12a6b2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for c in range(-9, 11):\n",
    "    C = 10**((c))\n",
    "    \n",
    "    for PS in [('saga', 'elasticnet'), \n",
    "               ('saga', 'l1'), ('saga', 'l2'), ('saga', 'none')]:\n",
    "    # for PS in [('newton-cg', 'l2'), ('newton-cg', 'none'), ('lbfgs', 'l2'), \n",
    "    #            ('lbfgs', 'none'), ('liblinear', 'l1'), ('liblinear', 'l2'), \n",
    "    #            ('sag', 'l2'), ('sag', 'none'), ('saga', 'elasticnet'), \n",
    "    #            ('saga', 'l1'), ('saga', 'l2'), ('saga', 'none')]:\n",
    "\n",
    "        switch = True\n",
    "        i = 0\n",
    "        X = copy.deepcopy(X_)\n",
    "        y = copy.deepcopy(Y_)\n",
    "\n",
    "        while switch:\n",
    "            X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "            X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "            X2.fit(X_train, y_train)\n",
    "            X_train = X2.transform(X_train)\n",
    "            X_val = X2.transform(X_val)\n",
    "            X_test = X2.transform(X_test)\n",
    "\n",
    "\n",
    "            clf = LogisticRegression(penalty=PS[1], C = C, solver = PS[0], l1_ratio = 1)\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            cent_storage_cols = {'C': [C], 'penalty': [PS[1]], 'solver': [PS[0]], \n",
    "                                 'fs':[int(X.shape[1]*0.25)+i], \n",
    "                            'train_accuracy': [clf.score(X_train, y_train)], \n",
    "                                 'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                                'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "\n",
    "            Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "            Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "\n",
    "            i += 1000\n",
    "            if int(X.shape[1]*0.25)+i >= X.shape[1]:\n",
    "                switch = False\n",
    "\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c7bd9-4cda-4a87-b5f9-93ea362ece08",
   "metadata": {},
   "source": [
    "Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd81dc30-7a29-4918-b04f-d6e4e6bd548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'LR'\n",
    "DTYPE = 'TFIDF_Preprocessed'\n",
    "\n",
    "cent_storage_cols = {'alpha': [], 'penalty': [], 'solver': [], 'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "748fd275-105d-42ac-b801-66a3165c63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for c in range(-3, 10):\n",
    "    C = 10**(-(c))\n",
    "    \n",
    "    for PS in [('lbfgs', 'l2'), \n",
    "               ('lbfgs', 'none')]:\n",
    "\n",
    "        X = copy.deepcopy(X_)\n",
    "        y = copy.deepcopy(Y_)\n",
    "\n",
    "        X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "        # X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "        # X2.fit(X_train, y_train)\n",
    "        # X_train = X2.transform(X_train)\n",
    "        # X_val = X2.transform(X_val)\n",
    "\n",
    "        clf = LogisticRegression(penalty=PS[1], C = C, solver = PS[0], max_iter = 5000)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        cent_storage_cols = {'C': [C], 'penalty': [PS[1]], 'solver': [PS[0]], \n",
    "                                'train_accuracy': [clf.score(X_train, y_train)], \n",
    "                                     'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                                    'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "\n",
    "        Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "        Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87058cdf-c93f-4c74-b419-fff2aca829e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82845d11-a0ba-4b1b-8308-c5743484f54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6195718654434251"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_Run = RandomForestClassifier(criterion = 'gini', n_estimators = 50, max_depth = 10, max_features = 0.5, max_samples = 0.25)\n",
    "\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X, Y, train_size = 0.7, random_state = 30027)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "\n",
    "RF_Run.fit(X_train, y_train)\n",
    "accuracy_score(y_val, RF_Run.predict(X_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2397ccc-e704-448c-ae37-4754dc39e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6293165585479327"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, RF_Run.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab06aa-4dca-4f3e-9510-1d972e1076e4",
   "metadata": {},
   "source": [
    "With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c8b847-ff48-4add-9a8e-83e711f0a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "MODEL_NAME = 'RF'\n",
    "DTYPE = 'W2V3000'\n",
    "\n",
    "cent_storage_cols = {'crit': [], 'n_est': [], 'max_depth': [], \n",
    "                        'max_feat': [], 'max_sampl': [], 'fs':[], \n",
    "                                'train_accuracy': [], 'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33f995-3e66-4f02-8253-b59a13685d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for crit in ('entropy',):\n",
    "    for n_est in (100,):\n",
    "        for max_depth in (4, 7, 10):\n",
    "            for max_feat in (0.5, 0.75):\n",
    "                for max_sample in (0.5, 0.75):\n",
    "                    \n",
    "                    switch = True\n",
    "                    i = 0\n",
    "                    X = copy.deepcopy(X_)\n",
    "                    y = copy.deepcopy(Y_)\n",
    "\n",
    "\n",
    "                    while switch:\n",
    "                        X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "                        X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "                        \n",
    "                        X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "                        X2.fit(X_train, y_train)\n",
    "                        \n",
    "                        X_train = X2.transform(X_train)\n",
    "                        X_val = X2.transform(X_val)\n",
    "                        X_test = X2.transform(X_test)\n",
    "                        \n",
    "                        clf = RandomForestClassifier(criterion = crit, n_estimators = n_est, max_samples = max_sample, max_features = max_feat, max_depth = max_depth)\n",
    "                        clf.fit(X_train, y_train)\n",
    "                        \n",
    "                        cent_storage_cols = {'crit': [crit], 'n_est': [n_est], 'max_depth': [max_depth], \n",
    "                                                             'max_feat': [max_feat], 'max_sampl': [max_sample], 'fs':[int(X.shape[1]*0.25)+i], \n",
    "                                                             'train_accuracy': [clf.score(X_train, y_train)], 'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                                                                'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "                        \n",
    "                        Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "                        Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "                        \n",
    "                        switch = False\n",
    "                        # i += 1000\n",
    "                        # if int(X.shape[1]*0.25)+i >= X.shape[1]:\n",
    "                        #     switch = False\n",
    "\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962367d-7cd3-4a3d-bcbe-22c70decb2fd",
   "metadata": {},
   "source": [
    "Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "853e1a92-2b6f-413a-a45b-5289a294d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'RF'\n",
    "DTYPE = 'W2V3000'\n",
    "\n",
    "cent_storage_cols = {'crit': [], 'n_est': [], 'max_depth': [], \n",
    "                        'max_feat': [], 'max_sampl': [], 'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102642f0-2b0e-4b1a-b9de-eee0670ef53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for crit in ('entropy', 'gini'):\n",
    "    for n_est in (100,):\n",
    "        for max_depth in (4, 7, 10):\n",
    "            for max_feat in (0.5, 0.75):\n",
    "                for max_sample in (0.5, 0.75):\n",
    "                    \n",
    "                    X = copy.deepcopy(X_)\n",
    "                    y = copy.deepcopy(Y_)\n",
    "\n",
    "\n",
    "                    \n",
    "                    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "                    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "                    # X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "                    # X2.fit(X_train, y_train)\n",
    "                    # X_train = X2.transform(X_train)\n",
    "                    # X_val = X2.transform(X_val)\n",
    "                    clf = RandomForestClassifier(criterion = crit, n_estimators = n_est, max_samples = max_sample, max_features = max_feat, max_depth = max_depth)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    cent_storage_cols = {'crit': [crit], 'n_est': [n_est], 'max_depth': [max_depth], \n",
    "                                                         'max_feat': [max_feat], 'max_sampl': [max_sample], \n",
    "                                                         'train_accuracy': [clf.score(X_train, y_train), \n",
    "                                                        'val_accuracy': [clf.score(X_val, y_val)], 'test_accuracy':[clf.score(X_test, y_test)}\n",
    "\n",
    "                    Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "                    Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "                        \n",
    "\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bb1c5-e341-49ae-aa76-c11ca6397b89",
   "metadata": {},
   "source": [
    "## 3. XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd76ab4-1bcb-4a8e-8619-8c35f9576015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6045871559633027"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_run = xgb.XGBClassifier(booster = 'gbtree', gamma = 0.01, subsample = 0.75, colsample_bytree = 0.75, max_depth = 10, eta = 0.25)\n",
    "\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X_, Y_, train_size = 0.7, random_state = 30027)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "\n",
    "y_train = y_train.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\n",
    "y_val = y_val.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\n",
    "\n",
    "xgb_run.fit(X_train, y_train)\n",
    "accuracy_score(y_val, xgb_run.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f811afe2-79f8-4044-81b3-9863c7987d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996068152031454"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, xgb_run.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31af31c-95b7-437d-8249-c2fd99a0af38",
   "metadata": {},
   "source": [
    "With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3f667f0-5acd-447a-b0d4-214bdc27e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'XGB'\n",
    "DTYPE = 'W2V3000'\n",
    "\n",
    "cent_storage_cols = {'gamma': [], 'sub_sample': [], 'col_sample_by_tree': [], \n",
    "                             'max_depth': [], 'eta':[], 'fs':[], 'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77f5baa-3211-454f-9fc3-991202deec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for gamma in (10, 1, 0.1):\n",
    "    for sub_sample in (0.5, 0.75):\n",
    "        for col_sample_by_tree in (0.5, 0.75):\n",
    "            for max_depth in (4, 7, 10):\n",
    "                for eta in (0.2,):\n",
    "\n",
    "                    switch = True\n",
    "                    i = 0\n",
    "                    X = copy.deepcopy(X_)\n",
    "                    y = copy.deepcopy(Y_)\n",
    "                    y = y.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\n",
    "\n",
    "                    while switch:\n",
    "                        X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "                        X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "                        # X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "                        # X2.fit(X_train, y_train)\n",
    "                        # X_train = X2.transform(X_train)\n",
    "                        # X_val = X2.transform(X_val)\n",
    "                        # X_test = X2.transform(X_test)\n",
    "    \n",
    "    \n",
    "                        clf = xgb.XGBClassifier(booster = 'gbtree', gamma = gamma, subsample = sub_sample, colsample_bytree = col_sample_by_tree,\n",
    "                                               max_depth = max_depth, eta = eta)\n",
    "                        clf.fit(X_train, y_train)\n",
    "                        cent_storage_cols = {'gamma': [gamma], 'sub_sample': [sub_sample], 'col_sample_by_tree': [col_sample_by_tree], \n",
    "                             'max_depth': [max_depth], 'eta':[eta], 'fs':[int(X.shape[1]*0.25)+i], \n",
    "                                        'train_accuracy': [clf.score(X_train, y_train)], \n",
    "                                             'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                                            'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "\n",
    "                        Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "                        Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "                        \n",
    "                        switch = False\n",
    "                        # i += 2500\n",
    "                        # if int(X.shape[1]*0.25)+i >= X.shape[1]:\n",
    "                        #     switch = False\n",
    "\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca3b3f-c9be-487a-a82e-e76576fd5964",
   "metadata": {},
   "source": [
    "Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f4a4b-b352-485a-8456-ccc60b2409e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'XGB'\n",
    "DTYPE = \n",
    "\n",
    "cent_storage_cols = {'gamma': [], 'sub_sample': [], 'col_sample_by_tree': [], \n",
    "                             'max_depth': [], 'eta':[], 'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa57ec-35b0-4173-99f4-42b48bd7fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for gamma in (0, 0.01, 0.05, 0.1, 0.5, 1):\n",
    "    for sub_sample in (0.25, 0.5, 0.75):\n",
    "        for col_sample_by_tree in (0.25, 0.5, 0.75):\n",
    "            for max_depth in (4, 6, 8, 10):\n",
    "                for eta in (0.1, 0.15, 0.2, 0.25, 0.3):\n",
    "\n",
    "                    X = copy.deepcopy(X_train_W2V)\n",
    "                    y = copy.deepcopy(Y_train_processed)\n",
    "                    y = y.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\n",
    "\n",
    "                    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "                    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "                    # X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "                    # X2.fit(X_train, y_train)\n",
    "                    # X_train = X2.transform(X_train)\n",
    "                    # X_val = X2.transform(X_val)\n",
    "\n",
    "                    clf = xgb.XGBClassifier(booster = 'gbtree', gamma = gamma, subsample = sub_sample, colsample_bytree = col_sample_by_tree,\n",
    "                                           max_depth = max_depth, eta = eta)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    cent_storage_cols = {'gamma': [gamma], 'sub_sample': [sub_sample], 'col_sample_by_tree': [col_sample_by_tree], \n",
    "                         'max_depth': [max_depth], 'eta':[eta], 'train_accuracy': [clf.score(X_train, y_train)], \n",
    "                                 'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                                'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "\n",
    "                    Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "                    Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a03d0-9ec7-4a58-8d5a-df4803566540",
   "metadata": {},
   "source": [
    "## 4. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "720fcede-e348-49f7-85cc-114edc1b4028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5241590214067279"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_run = svm.SVC(C= 10, decision_function_shape= 'ovo', degree= 1, gamma= 0.1, kernel= 'poly')\n",
    "\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X, Y, train_size = 0.7, random_state = 30027)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "\n",
    "svm_run.fit(X_train, y_train)\n",
    "accuracy_score(y_val, svm_run.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3b14425-b4db-4808-be6f-eb55f2f2158f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9836828309305373"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, svm_run.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935318a-91eb-42bb-b2aa-79be35291cd4",
   "metadata": {},
   "source": [
    "With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcb96f-dad8-4344-9b6b-06bb9a041540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'SVM'\n",
    "DTYPE = 'W2V3000'\n",
    "\n",
    "cent_storage_cols = {'C': [], 'kernel': [], 'gamma': [], 'degree': [], 'fs':[], 'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7be361-f939-4c28-9c21-e68665177bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for c in range(-3, 11):\n",
    "    C = 10**(-(c))\n",
    "    for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "        for g in range(-3,11):\n",
    "            gamma = 10**(-(g))\n",
    "            \n",
    "            for j in range(1,6):\n",
    "                if kernel != 'linear':\n",
    "                    u = j\n",
    "                    if u>=2:\n",
    "                        break\n",
    "                    else:\n",
    "                        u = None\n",
    "\n",
    "                switch = True\n",
    "                i = 0\n",
    "                X = copy.deepcopy(X_)\n",
    "                y = copy.deepcopy(Y_)\n",
    "\n",
    "                while switch:\n",
    "                    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "                    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "                    # X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "                    # X2.fit(X_train, y_train)\n",
    "                    # X_train = X2.transform(X_train)\n",
    "                    # X_val = X2.transform(X_val)\n",
    "                    # X_test = X2.transform(X_test)\n",
    "\n",
    "                    clf = SVC(C = C, kernel = kernel, gamma = gamma, degree = u)\n",
    "\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    cent_storage_cols = {'C': [C], 'kernel': [kernel], 'gamma': [gamma], 'degree': [u], \n",
    "                                         'fs':[int(X.shape[1]*0.25)+i], \n",
    "                                            'train_accuracy': [clf.score(X_train, y_train)], \n",
    "                                                 'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                                                'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "\n",
    "                    Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "                    Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "                    \n",
    "                    switch = False\n",
    "                    # i += 1000\n",
    "                    # if int(X.shape[1]*0.25)+i >= X.shape[1]:\n",
    "                    #     switch = False\n",
    "\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d44fe0-643a-4153-b03c-84bac3217f35",
   "metadata": {},
   "source": [
    "Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e04ba-85c5-4f80-9b40-fb4caed19189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'SVM'\n",
    "DTYPE = \n",
    "\n",
    "cent_storage_cols = {'C': [C], 'kernel': [kernel], 'gamma': [gamma], 'degree': [i], \n",
    "                     'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5f179-78a8-49aa-bcb3-a73343f1c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for c in range(-10, 11):\n",
    "    C = 10**(-(c))\n",
    "    for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "        for g in range(-10,11):\n",
    "            gamma = 10**(-(g))\n",
    "            \n",
    "            for i in range(1,10):\n",
    "            \n",
    "\n",
    "                X = copy.deepcopy(X_)\n",
    "                y = copy.deepcopy(Y_)\n",
    "\n",
    "                X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "                X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "                # X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "                # X2.fit(X_train, y_train)\n",
    "                # X_train = X2.transform(X_train)\n",
    "                # X_val = X2.transform(X_val)\n",
    "\n",
    "                clf = xgb.XGBClassifier(booster = 'gbtree', gamma = gamma, subsample = sub_sample, colsample_bytree = col_sample_by_tree,\n",
    "                                       max_depth = max_depth, eta = eta)\n",
    "                clf.fit(X_train, y_train)\n",
    "                cent_storage_cols = {'C': [C], 'kernel': [kernel], 'gamma': [gamma], 'degree': [i], \n",
    "                                            'train_accuracy': [clf.score(X_train, y_train)], \n",
    "                                                 'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                                                'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "\n",
    "                Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "                Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8dcf1-732b-4bf4-a553-d284c8574370",
   "metadata": {},
   "source": [
    "## 5. Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701145e9-c2f1-4f25-a1e9-6dda14421e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_run = BNB(alpha=1.0, fit_prior=True)\n",
    "\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X_, Y_, train_size = 0.7, random_state = 30027)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "\n",
    "# y_train = y_train.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\n",
    "# y_val = y_val.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\n",
    "\n",
    "bnb_run.fit(X_train, y_train)\n",
    "accuracy_score(y_val, bnb_run.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b28aec-a7e6-4e6c-baae-d24818d5a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, bnb_run.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ecece-6575-43f4-92e2-ce35fcede1ae",
   "metadata": {},
   "source": [
    "With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b9484e-9c3c-4971-a5f9-f2789a2bc5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'BNB'\n",
    "DTYPE = 'W2V3000'\n",
    "\n",
    "cent_storage_cols = {'alpha': [], 'fs':[], 'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e0b8988-0352-4e73-8041-341fdd6e6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Central_Statistics = pd.read_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv')\n",
    "for a in range(-2, 11):\n",
    "    alpha = 10**(-(a))\n",
    "\n",
    "    switch = True\n",
    "    i = 0\n",
    "    X = copy.deepcopy(X_)\n",
    "    y = copy.deepcopy(Y_)\n",
    "\n",
    "    while switch:\n",
    "        X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "        # X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "        # X2.fit(X_train, y_train)\n",
    "        # X_train = X2.transform(X_train)\n",
    "        # X_val = X2.transform(X_val)\n",
    "        # X_test = X2.transform(X_test)\n",
    "\n",
    "\n",
    "        clf = BNB(alpha=alpha, fit_prior=True)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        cent_storage_cols = {'alpha': [alpha], 'fs':[int(X.shape[1]*0.25)+i], \n",
    "                        'train_accuracy': [clf.score(X_train, y_train)], \n",
    "                             'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                            'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "\n",
    "        Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "        Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "        \n",
    "        switch = False\n",
    "        # i += 1000\n",
    "        # if int(X.shape[1]*0.25)+i >= X.shape[1]:\n",
    "        #     switch = False\n",
    "\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c00432-e02d-4d0e-92a8-433d87cc480d",
   "metadata": {},
   "source": [
    "Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65125d1d-2034-4066-a4d1-3cd2e016d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of empty lists which is a placeholder for each of the 40-ish columns\n",
    "\n",
    "MODEL_NAME = 'BNB'\n",
    "DTYPE = \n",
    "\n",
    "cent_storage_cols = {'alpha': [], 'train_accuracy': [],\n",
    "                    'val_accuracy': [], 'test_accuracy':[]}\n",
    "\n",
    "# Create DataFrame and export\n",
    "\n",
    "central_data = pd.DataFrame(cent_storage_cols)\n",
    "central_data.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e71ce-b1db-4a9b-a10d-7a7f81226ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Central_Statistics = pd.read_csv(f'Tuning {MODEL_NAME} {DTYPE}.csv')\n",
    "for a in range(-10, 11):\n",
    "    alpha = 10**(-(a))\n",
    "\n",
    "    X = copy.deepcopy(X_)\n",
    "    y = copy.deepcopy(Y_)\n",
    "    y = y.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\n",
    "\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "    # X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "    # X2.fit(X_train, y_train)\n",
    "    # X_train = X2.transform(X_train)\n",
    "    # X_val = X2.transform(X_val)\n",
    "\n",
    "    clf = BNB(alpha=alpha, fit_prior=True)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    cent_storage_cols = {'alpha': [alpha], \n",
    "                            'train_accuracy': [clf.score(X_train, y_train)], \n",
    "                                 'val_accuracy': [clf.score(X_val, y_val)],\n",
    "                                'test_accuracy': [clf.score(X_test, y_test)]}\n",
    "\n",
    "    Central_Statistics = Central_Statistics.append(pd.DataFrame(cent_storage_cols))\n",
    "\n",
    "    Central_Statistics.to_csv(f'./Tuning/Tuning - {MODEL_NAME} {DTYPE}.csv', index = False)\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c24f0-f143-44aa-b236-a22ca559e534",
   "metadata": {},
   "source": [
    "# Early Code for determining which preprocessing combination was best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec9b3b-1b5c-46f9-9e4b-bb3eddd319b3",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7a7f4c-59bb-48fa-8a32-6d39fecb5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accu_score = list()\n",
    "# X = copy.deepcopy(X_train_processed)\n",
    "# y = copy.deepcopy(Y_train_processed)\n",
    "\n",
    "# X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "# clf = svm.SVC(kernel = 'linear')\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# accu_score.append(clf.score(X_val, y_val))\n",
    "\n",
    "        \n",
    "\n",
    "# print(\"SVM\")\n",
    "# print(accu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02915d3f-3426-49a1-a577-95eb796b0081",
   "metadata": {},
   "source": [
    "## LogR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30cc040-029f-4650-a326-466508c32dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.decomposition import PCA\n",
    "# accu_score = list()\n",
    "\n",
    "# X = copy.deepcopy(X_train_processed)\n",
    "# y = copy.deepcopy(Y_train_processed)\n",
    "\n",
    "# X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "\n",
    "# # lgr = LogisticRegression(max_iter = 500, penalty = 'l1', solver = 'saga', C = 2, multi_class = 'multinomial')\n",
    "# # 0.67217125382263 accuracy just playing arround\n",
    "# lgr = LogisticRegression(max_iter = 500, penalty = 'l2', solver = 'sag', C = 2)\n",
    "# lgr.fit(X_train, y_train)\n",
    "# accu_score.append(lgr.score(X_val, y_val))\n",
    "\n",
    "# print('Logistic without feature selection')\n",
    "# print(accu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64306e-2e7e-4de1-be0b-ec2eb93a77ab",
   "metadata": {},
   "source": [
    "## RANDOM FOREST WITHOUT FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e22bf7-e5f1-45ce-adda-977c12b08d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# accu_score = list()\n",
    "\n",
    "# X = copy.deepcopy(X_train_processed)\n",
    "# y = copy.deepcopy(Y_train_processed)\n",
    "\n",
    "# X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "\n",
    "# clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators = 10, max_samples = 0.5, max_features = 0.5)\n",
    "# clf.fit(X_train, y_train)\n",
    "# accu_score.append(clf.score(X_val, y_val))\n",
    "\n",
    "# print('Random Forest without feature selection')\n",
    "# print(accu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743068a-e3ca-47e3-8703-b7313ad8554d",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6501bb5-89f2-482b-8268-cb371347d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# import copy\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# accu_score = list()\n",
    "\n",
    "# X = copy.deepcopy(X_train_processed)\n",
    "# y = copy.deepcopy(Y_train_processed)\n",
    "# y = y.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\n",
    "\n",
    "\n",
    "# X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "\n",
    "# clf = xgb.XGBClassifier(booster = 'gbtree')\n",
    "# clf.fit(X_train, y_train)\n",
    "# accu_score.append(clf.score(X_val, y_val))\n",
    "   \n",
    "\n",
    "\n",
    "# print(\"XGB without feature selection\")\n",
    "# max_value = max(accu_score)\n",
    "# max_index = accu_score.index(max_value)\n",
    "# print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91922fc-5f65-471a-aed3-ab8a7d566eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
