{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6025ce58-8f20-4d9d-a2bd-9062527cee4a",
   "metadata": {},
   "source": [
    "# 2. Tuning Part 2\n",
    "\n",
    "### for SVM and Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d6a77b-f110-42a2-959f-866989fe4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a178e44-2fc3-4805-81dc-5517c117f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Phase1.csv')\n",
    "df = pd.read_csv('Phase1_1.csv')\n",
    "df1 = pd.read_csv('Phase1_1test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82416f10-4d40-44c0-9128-b0ef22229eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = list()\n",
    "for tweet in df['text']:\n",
    "    \n",
    "    tmp = list()\n",
    "    \n",
    "    # get rid of non-alphanumerical chars, effectively splitting them\n",
    "    # except for at ' - # and %\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F#%\\'-]', ' ', tweet)\n",
    "\n",
    "    # tweet = re.sub(r'[^\\w%\\'-]', ' ', tweet)\n",
    "                \n",
    "#     # get rid of non-english characters\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F ]+', ' ', tweet) \n",
    "    \n",
    "#     # ??? 是不是有问题\n",
    "    \n",
    "    # split the words based on space\n",
    "    words = tweet.split()\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        # get rid of ' and % and - that are outside of words\n",
    "        word = word.strip('\\'%-')\n",
    "        \n",
    "        # get rid of ' in all words\n",
    "        # CURRENTLY KEEPS - in words\n",
    "        word = re.sub(r'[^\\w#-]', '', word)\n",
    "        \n",
    "        # word = re.sub(r'[^\\w-]', '', word)\n",
    "        \n",
    "        if word not in ['', '#']:\n",
    "            tmp.append(word)\n",
    "    text_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41c0e8f-0b70-4312-916f-2ab426ee51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = text_list\n",
    "df.to_csv('phase2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74761056-d92d-45a7-9d17-b8470a7f4e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>802217876644052000</td>\n",
       "      <td>[putin, abducted, ukrainian, citizens, in, occ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802425296955682000</td>\n",
       "      <td>[if, putin, wanted, to, intervene, all, he, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>805664502515662000</td>\n",
       "      <td>[elsewhere, in, progland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>639928670103015000</td>\n",
       "      <td>[any, females, copping, the, gucci, foamposite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>673824182287904000</td>\n",
       "      <td>[in, other, news, had, a, great, saturday, on,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>662664783083122000</td>\n",
       "      <td>[i, have, tickets, to, see, the, vamps, on, ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>635582180316479000</td>\n",
       "      <td>[you, may, have, seen, this, but, john, cena, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>805537394262994000</td>\n",
       "      <td>[this, is, what, democracy, should, look, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>640586501571637000</td>\n",
       "      <td>[damn, shawn, was, right, i, may, have, to, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>802428434588827000</td>\n",
       "      <td>[white, liberals, make, me, sick, like, you, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6099 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                               text\n",
       "0     802217876644052000  [putin, abducted, ukrainian, citizens, in, occ...\n",
       "1     802425296955682000  [if, putin, wanted, to, intervene, all, he, wo...\n",
       "2     805664502515662000                          [elsewhere, in, progland]\n",
       "3     639928670103015000  [any, females, copping, the, gucci, foamposite...\n",
       "4     673824182287904000  [in, other, news, had, a, great, saturday, on,...\n",
       "...                  ...                                                ...\n",
       "6094  662664783083122000  [i, have, tickets, to, see, the, vamps, on, ap...\n",
       "6095  635582180316479000  [you, may, have, seen, this, but, john, cena, ...\n",
       "6096  805537394262994000  [this, is, what, democracy, should, look, like...\n",
       "6097  640586501571637000  [damn, shawn, was, right, i, may, have, to, co...\n",
       "6098  802428434588827000  [white, liberals, make, me, sick, like, you, a...\n",
       "\n",
       "[6099 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### THIS IS FOR THE PROCESSING OF TESTING DATA (remove punctuation) IF NEEDED DEPENDING ON OUR DECISION OF \n",
    "# USING BAILAN OR PREPROCESS\n",
    "text_list = list()\n",
    "for tweet in df1['text']:\n",
    "    \n",
    "    tmp = list()\n",
    "    \n",
    "    # get rid of non-alphanumerical chars, effectively splitting them\n",
    "    # except for at ' - # and %\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F#%\\'-]', ' ', tweet)\n",
    "\n",
    "    # tweet = re.sub(r'[^\\w%\\'-]', ' ', tweet)\n",
    "                \n",
    "#     # get rid of non-english characters\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F ]+', ' ', tweet) \n",
    "    \n",
    "#     # ??? 是不是有问题\n",
    "    \n",
    "    # split the words based on space\n",
    "    words = tweet.split()\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        # get rid of ' and % and - that are outside of words\n",
    "        word = word.strip('\\'%-')\n",
    "        \n",
    "        # get rid of ' in all words\n",
    "        # CURRENTLY KEEPS - in words\n",
    "        word = re.sub(r'[^\\w#-]', '', word)\n",
    "        \n",
    "        # word = re.sub(r'[^\\w-]', '', word)\n",
    "        \n",
    "        if word not in ['', '#']:\n",
    "            tmp.append(word)\n",
    "    text_list.append(tmp)\n",
    "df1['text'] = text_list\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f96eb-167e-4716-9dcd-2d2a198f6e69",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2c4a79-6c73-481b-beae-7b9926402d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# stemmeddf = df.copy(deep=True)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    return [ps.stem(w) for w in text]\n",
    "\n",
    "# stemmeddf['text'] = stemmeddf.text.apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d840a954-9489-4364-93c2-af6245ae4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmeddf.to_csv('stemmedPhase2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cef51-2c69-4d5c-9caa-03a4189997eb",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6a5359-677c-4b62-a75d-12af144cbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#rmstopworddf = df.copy(deep=True)\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if not w in stop_words]\n",
    "\n",
    "#rmstopworddf['text'] = rmstopworddf.text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1658c5a3-6455-4f6d-bc86-676950b5a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmstopworddf.to_csv('rmStopwordsPhase2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cfa5f8-756c-41ea-86c6-5c396d3cf033",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a60a09e-5765-45d6-a9fa-c25b39538a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Build the feature set (vocabulary) and vectorise the Tarin dataset using TFIDF\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(df_for_vectorize['text'])\n",
    "\n",
    "\n",
    "# print(\"Train feature space size (using TFIDF):\",X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d674da5d-ddfd-47bf-b70b-0db6c9906539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca5995f-f7da-486c-9211-5b5cf5507a4d",
   "metadata": {},
   "source": [
    "### dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec2c510-8b1c-4da3-b34c-715443b066fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doctor hit campaign trail as race to medic council elect heat up homeopathi\n",
      "(21801, 21734)\n"
     ]
    }
   ],
   "source": [
    "# BoW_vectorizer = CountVectorizer(lowercase=False, ngram_range=(1,1))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "def join_string(text):\n",
    "    return ' '.join(text)\n",
    "def remove_hashtag(text):\n",
    "    return [w.strip(\"#\") for w in text]\n",
    "\n",
    "dfcopy = df.copy(deep = True)\n",
    "# dfcopy['text'] = dfcopy['text'].apply(remove_stopwords)\n",
    "dfcopy['text'] = dfcopy['text'].apply(remove_hashtag)\n",
    "dfcopy['text'] = dfcopy['text'].apply(stem_text)\n",
    "dfcopy['text'] = dfcopy['text'].apply(join_string)\n",
    "print(dfcopy['text'][0])\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_train_processed = tfidf_vectorizer.fit_transform(dfcopy['text'])\n",
    "Y_train_processed = dfcopy['sentiment']\n",
    "print(X_train_processed.shape)\n",
    "\n",
    "#UNPROCESSED only tfidf\n",
    "dfcopy1 = pd.read_csv(\"Train.csv\")\n",
    "#tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_UN = tfidf_vectorizer.fit_transform(dfcopy1['text'])\n",
    "Y_train_UN = dfcopy1['sentiment']\n",
    "\n",
    "X_word_2vec = pd.read_csv(\"W2V_Train.csv\")\n",
    "# # Expand contraction new function in phase 1\n",
    "# # Xtra processed\n",
    "# dfcopy2 = df1.copy(deep = True)\n",
    "# dfcopy2['text'] = dfcopy2['text'].apply(stem_text)\n",
    "# # dfcopy['text'] = dfcopy['text'].apply(remove_stopwords)\n",
    "# dfcopy2['text'] = dfcopy2['text'].apply(join_string)\n",
    "# tfidf_vectorizer = TfidfVectorizer(lowercase=False, ngram_range=(1,1))\n",
    "# X_train_Xprocessed = tfidf_vectorizer.fit_transform(dfcopy2['text'])\n",
    "# Y_train_Xprocessed = dfcopy2['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ead6e7-7553-446f-af58-44d7e0fdd36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21800, 99)\n",
      "(21801, 21734)\n"
     ]
    }
   ],
   "source": [
    "print(X_word_2vec.shape)\n",
    "print(X_train_processed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6cc1f-62cc-4840-a05e-b94bc5de41db",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da21426-2f4e-49d4-84db-b225d3a72ed8",
   "metadata": {},
   "source": [
    "## Logistice Reg tunning parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8e26dc-e870-448e-acd3-8b44cfe53d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "accu_score = list()\n",
    "lst = []\n",
    "switch = True\n",
    "i = 0\n",
    "X = copy.deepcopy(X_word_2vec)\n",
    "y = copy.deepcopy(Y_train_processed)\n",
    "while switch:\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "    X2 = SelectKBest(chi2, k=int(X.shape[1]*0.25)+i)\n",
    "    X2.fit(X_train, y_train)\n",
    "    X_train = X2.transform(X_train)\n",
    "    X_val = X2.transform(X_val)\n",
    "    print('a')\n",
    "    parameter_space = {\n",
    "        'penalty': ['l2', 'none', 'l1', 'elasticnet'],\n",
    "        'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2],\n",
    "        'solver': ['saga'],\n",
    "        'multi_class': ['auto', 'ovr', 'multinomial']\n",
    "    }\n",
    "    lgr = LogisticRegression(max_iter = 1000)\n",
    "    clf = GridSearchCV(lgr, parameter_space, n_jobs=-1, cv=2)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print('Best parameters found:\\n', clf.best_params_)\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, params in zip(means, clf.cv_results_['params']):\n",
    "        lst.append((((X.shape[1]*0.25)+i, mean), params))\n",
    "    i += 1000\n",
    "    if int(X.shape[1]*0.25)+i >= X.shape[1]:\n",
    "        switch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5288a63-a306-42cc-ade2-971ccfc2f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec9b3b-1b5c-46f9-9e4b-bb3eddd319b3",
   "metadata": {},
   "source": [
    "## SVM tunning parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a7f4c-59bb-48fa-8a32-6d39fecb5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "accu_score = list()\n",
    "X = copy.deepcopy(X_train_processed)\n",
    "y = copy.deepcopy(Y_train_processed)\n",
    "\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size = 0.7, random_state = 30027)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size = 0.5, random_state = 30027)\n",
    "svm = svm.SVC()\n",
    "parameter_space = {'C': [0.1,1, 10, 100], \n",
    "                    'gamma': [1,0.1,0.01,0.001],\n",
    "                    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                    'degree':[1,2,3,4,5,6,7,8,9,10],\n",
    "                    'decision_function_shape':['ovo', 'ovr']\n",
    "                       \n",
    "                   }\n",
    "\n",
    "clf = GridSearchCV(svm, parameter_space, n_jobs=-1, cv=2)\n",
    "print('a')\n",
    "clf.fit(X_train, y_train)\n",
    "print('b')\n",
    "\n",
    "# {'C': 10, 'decision_function_shape': 'ovo', 'degree': 1, 'gamma': 0.1, 'kernel': 'poly'}\n",
    "        \n",
    "\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
